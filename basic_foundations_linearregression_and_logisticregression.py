# -*- coding: utf-8 -*-
"""Basic Foundations - LinearRegression and LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YSoYb36tUu4QmXMJvvDEdSPNijU0yach
"""

import numpy as np
import matplotlib.pyplot as plt

X = np.array([1,2,3,4,5])
Y = np.array([3,5,7,9,11])

m = 0
b = 0

learning_rate = 0.01
ephochs = 10

n = len(X)
losses = []

for i in range(ephochs):
  # We use current guess of m and b.
  y_pred = m * X + b
  # We check how wrong we are.
  loss = (1/n)*np.sum((Y-y_pred)**2)
  losses.append(loss)
  # Gradient - If prediction too small - increase m and vice versa
  dm = (-2/n) * np.sum(X*(Y-y_pred))
  db = (-2/n) * np.sum(Y-y_pred)
  # Move a little bit opposite slope.
  m = m - learning_rate * dm
  b = b - learning_rate * db
  # Visualize Learning
  plt.plot(losses)
  plt.title("Loss Decreasing Over Time")
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.show()

"""Logistic Regression"""

import numpy as np
import matplotlib.pyplot as plt

X = np.array([0.5,1,1.5,2,2.5,3])
Y = np.array([0,0,0,1,1,1])

w=0
b=0

learning_rate = 0.01
ephochs = 1000

def sigmoid(z):
  return 1 / (1+np.exp(-z))

for i in range(ephochs):
  z = w *X+b
  y_pred = sigmoid(z)
  # gradient
  dw = (1/n) * np.sum((y_pred - Y) * X)
  db = (1/n) * np.sum(y_pred - Y)

  w -= learning_rate *dw
  b-= learning_rate*db

print("Final w:", w)
print("Final b:", b)

